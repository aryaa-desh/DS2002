{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4c86d0-2b0d-4028-8c13-bc787cc943af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using python to perform ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9f82cb-cb55-42f4-930d-0308a970cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.12/site-packages (4.11.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymongo) (2.7.0)\n"
     ]
    }
   ],
   "source": [
    "%%python -m pip install pymongo\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6848a9-ae77-4345-a6b4-d0b4c2ddcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fe82c-5dc9-4154-b3bf-b3a22b0c3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d3a60c-65e9-4166-8816-17d0a817f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.34\n",
      "Running PyMongo Version: 4.11.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60da59d8-5084-4db2-8302-317bd325f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB: Declare and Assign Connection Variables for the MongoDB Server \n",
    "mysql_args = {\n",
    "    \"uid\" : \"root\",\n",
    "    \"pwd\" : \"Strawberrylime\",\n",
    "    \"hostname\" : \"localhost\",\n",
    "    \"dbname\" : \"classicmodels_dw\"\n",
    "}\n",
    "conn_str = f\"mysql+pymysql://{mysql_args['uid']}:{mysql_args['pwd']}@{mysql_args['hostname']}/{mysql_args['dbname']}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "# The 'cluster_location' must either be \"atlas\" or \"local\".\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"aryaa_desh\",\n",
    "    \"password\" : \"Strawberrylime\",\n",
    "    \"cluster_name\" : \"Cluster0\",\n",
    "    \"cluster_subnet\" : \"xxxxx\",\n",
    "    \"cluster_location\" : \"local\", # \"local\"\n",
    "    \"db_name\" : \"classicmodels_dw\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4101c5ca-1238-4143-8144-cce5ba95ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare & Assign Connection Variables for the MySQL Server & Databases with which You'll be Working\n",
    "host_name = \"localhost\"\n",
    "port = \"3306\"\n",
    "user_id = \"root\"\n",
    "pwd = \"Strawberrylime\"\n",
    "\n",
    "src_dbname = \"classicmodels\"\n",
    "dst_dbname = \"classicmodels_dw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1441c0-5d0b-4d43-9a68-9eaf9f409bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions for Getting Data From and Setting Data into Database for MongoDB\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(text(sql_query), connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name, data_directory, json_files):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for collection_name in json_files:\n",
    "        json_path = os.path.join(data_directory, json_files[collection_name])\n",
    "        records = []\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            for i, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # skip blank lines\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping line {i} due to error: {e}\")\n",
    "        \n",
    "        if records:\n",
    "            result = db[collection_name].insert_many(records)\n",
    "            print(f\"✅ Inserted {len(result.inserted_ids)} records into '{collection_name}'\")\n",
    "        else:\n",
    "            print(f\"⚠️ No valid records found for '{collection_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679167c8-d629-40f9-a10d-0ddbb4719f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into `orders` table in `classicmodels_dw` database.\n"
     ]
    }
   ],
   "source": [
    "# Define CSV File Path (Ensure the file is in the correct directory)\n",
    "data_dir = os.getcwd()  # Adjust if necessary\n",
    "data_file = os.path.join(data_dir, \"orders.csv\")\n",
    "\n",
    "# Read CSV into Pandas DataFrame\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Load Data into MySQL\n",
    "table_name = \"orders\"  # Ensure this matches your MySQL table name\n",
    "df.to_sql(name=table_name, con=sqlEngine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Data successfully inserted into `{table_name}` table in `{mysql_args['dbname']}` database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac909f8-b88f-4a3c-b1f4-a8a9d410bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate MongoDB with Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12f10e9-7799-4c03-887f-5cc40d5714ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t1.1.1.1\n",
      "Address:\t1.1.1.1#53\n",
      "\n",
      "** server can't find cluster_name.xxxxx.mongodb.net: NXDOMAIN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nslookup cluster_name.xxxxx.mongodb.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a45bf7-90f1-4710-9868-e4fb56e55349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/aryaadeshpande/Desktop\n",
      "Skipping line 1 due to error: Expecting value: line 1 column 2 (char 1)\n",
      "Skipping line 2 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 3 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 4 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 5 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 6 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 7 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 8 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 9 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 10 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 11 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 12 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 13 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 14 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 15 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 16 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 17 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 18 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 19 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 20 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 21 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 22 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 23 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 24 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 25 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 26 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 27 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 28 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 29 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 30 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 31 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 32 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 33 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 34 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 35 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 36 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 37 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 38 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 39 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 40 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 41 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 42 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 43 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 44 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 45 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 46 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 47 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 48 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 49 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 50 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 51 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 52 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 53 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 54 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 55 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 56 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 57 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 58 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 59 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 60 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 61 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 62 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 63 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 64 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 65 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 66 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 67 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 68 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 69 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 70 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 71 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 72 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 73 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 74 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 75 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 76 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 77 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 78 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 79 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 80 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 81 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 82 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 83 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 84 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 85 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 86 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 87 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 88 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 89 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 90 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 91 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 92 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 93 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 94 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 95 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 96 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 97 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 98 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 99 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 100 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 101 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 102 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 103 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 104 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 105 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 106 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 107 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 108 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 109 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 110 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 111 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 112 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 113 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 114 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 115 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 116 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 117 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 118 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 119 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 120 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 121 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 122 due to error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Skipping line 123 due to error: Extra data: line 1 column 17 (char 16)\n",
      "Skipping line 124 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 125 due to error: Extra data: line 1 column 14 (char 13)\n",
      "Skipping line 126 due to error: Extra data: line 1 column 9 (char 8)\n",
      "Skipping line 127 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping line 128 due to error: Expecting value: line 1 column 1 (char 0)\n",
      "⚠️ No valid records found for 'payments'\n"
     ]
    }
   ],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "# Gets the path of the Current Working Directory for this Notebook,\n",
    "# and then Appends the 'data' directory.\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "json_files = {\"payments\" : 'payments.json'}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf50ac2-63d4-4c2a-9e89-e2aefe6a5c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerNumber</th>\n",
       "      <th>checkNumber</th>\n",
       "      <th>paymentDate</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>HQ336336</td>\n",
       "      <td>2004-10-19</td>\n",
       "      <td>6066.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>JM555205</td>\n",
       "      <td>2003-06-05</td>\n",
       "      <td>14571.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerNumber checkNumber paymentDate    amount\n",
       "0             103    HQ336336  2004-10-19   6066.78\n",
       "1             103    JM555205  2003-06-05  14571.44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your query and collection first\n",
    "query = {}  # get all documents\n",
    "collection = \"payments\"\n",
    "\n",
    "# Extract Data from the Source MongoDV Collections into DataFrames\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  # Adjust the URI as needed\n",
    "db = client[\"db_name\"]\n",
    "\n",
    "dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "# query = {} # Select all elements (columns), and all documents (rows).\n",
    "# collection = \"payments\"\n",
    "\n",
    "df_payments = get_mongo_dataframe(client, mongodb_args[\"db_name\"], collection, query)\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4ec9e0b-b94e-4e4c-a0b3-3b5be336c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, run the [DS2002 Midterm dim_date SQL file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b619d2-f012-4413-a6b2-726c9226d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb50828d-452b-44f2-843e-caf8e7909767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000103</td>\n",
       "      <td>2000-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000104</td>\n",
       "      <td>2000-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000105</td>\n",
       "      <td>2000-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20000106</td>\n",
       "      <td>2000-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20000107</td>\n",
       "      <td>2000-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20000108</td>\n",
       "      <td>2000-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20000109</td>\n",
       "      <td>2000-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20000110</td>\n",
       "      <td>2000-01-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date\n",
       "0  20000101  2000-01-01\n",
       "1  20000102  2000-01-02\n",
       "2  20000103  2000-01-03\n",
       "3  20000104  2000-01-04\n",
       "4  20000105  2000-01-05\n",
       "5  20000106  2000-01-06\n",
       "6  20000107  2000-01-07\n",
       "7  20000108  2000-01-08\n",
       "8  20000109  2000-01-09\n",
       "9  20000110  2000-01-10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up the payments_date keys from the date dimension table\n",
    "sql_dim_date = \"SELECT date_key, full_date FROM classicmodels_dw.dim_date;\"\n",
    "df_dim_date = get_sql_dataframe(sql_dim_date, **mysql_args)\n",
    "df_dim_date.full_date = df_dim_date.full_date.astype('datetime64[ns]').dt.date\n",
    "df_dim_date.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f3209e4-5319-4bf8-8b8e-3da24a597b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerNumber</th>\n",
       "      <th>checkNumber</th>\n",
       "      <th>amount</th>\n",
       "      <th>paymentDate_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>HQ336336</td>\n",
       "      <td>6066.78</td>\n",
       "      <td>20041019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>JM555205</td>\n",
       "      <td>14571.44</td>\n",
       "      <td>20030605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerNumber checkNumber    amount  paymentDate_key\n",
       "0             103    HQ336336   6066.78         20041019\n",
       "1             103    JM555205  14571.44         20030605"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up the Surrogate Primary Key (date_key) that corresponds to the paymentDate column\n",
    "\n",
    "df_dim_paymentDate = df_dim_date.rename(columns={\"date_key\" : \"paymentDate_key\", \"full_date\" : \"paymentDate\"})\n",
    "df_payments.paymentDate = df_payments.paymentDate.astype('datetime64[ns]').dt.date\n",
    "df_payments = pd.merge(df_payments, df_dim_paymentDate, on='paymentDate', how='left')\n",
    "df_payments.drop(['paymentDate'], axis=1, inplace=True)\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1605ab2-bc97-493d-8225-031dfc3e7cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_key</th>\n",
       "      <th>customerNumber</th>\n",
       "      <th>amount</th>\n",
       "      <th>paymentDate_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>6066.78</td>\n",
       "      <td>20041019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>14571.44</td>\n",
       "      <td>20030605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_key  customerNumber    amount  paymentDate_key\n",
       "0            1             103   6066.78         20041019\n",
       "1            2             103  14571.44         20030605"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Any Necessary Transformations to the DataFrames\n",
    "\n",
    "# 3. Insert a new column, with an ever-incrementing numeric value, to serve as the primary key.\n",
    "df_payments.drop(['checkNumber'], axis=1, inplace=True)\n",
    "df_payments.insert(0, \"payment_key\", range(1, df_payments.shape[0]+1))\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "296d0779-08bb-428e-b781-d6ccc46cc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformed payments DataFrames into the New Data Warehouse by creating new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33a957-5072-445e-a348-0e51dfa6972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1aef347-2e45-49e7-8452-154b9d47f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Functions for Getting Data From and Setting Data Into Databases\n",
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "\n",
    "    # Debugging the query and connection\n",
    "    print(f\"Inserting into {table_name} with primary key {pk_column}\")\n",
    "\n",
    "    # Safely insert or update data\n",
    "    if db_operation == \"insert\":\n",
    "        try:\n",
    "            df.to_sql(table_name, con=connection, index=False, if_exists='append')  # Append data without replacing\n",
    "            print(\"Data inserted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "        # Ensure the primary key column exists with AUTO_INCREMENT before inserting\n",
    "        try:\n",
    "            # Check if the column exists\n",
    "            result = connection.execute(f\"SHOW COLUMNS FROM {table_name} LIKE '{pk_column}';\")\n",
    "            if not result.fetchone():\n",
    "                print(f\"Adding primary key column {pk_column} as AUTO_INCREMENT.\")\n",
    "                # Add primary key if it doesn't exist\n",
    "                connection.execute(f\"ALTER TABLE {table_name} ADD COLUMN {pk_column} INT AUTO_INCREMENT PRIMARY KEY;\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding primary key to {table_name}: {e}\")\n",
    "\n",
    "    elif db_operation == \"replace\":\n",
    "        try:\n",
    "            df.to_sql(table_name, con=connection, index=False, if_exists='replace')  # ⚠️ Use this carefully!\n",
    "            connection.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            print(\"Data replaced successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error replacing data into {table_name}: {e}\")\n",
    "\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e417712-581f-4d57-9480-a3bfbdf8e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into dim_payments with primary key payment_key\n"
     ]
    }
   ],
   "source": [
    "# TODO: Upload the \"payments\" dataframe to create the new \"dim_payments\" dimension table\n",
    "# Check for duplicates in the payment_key column\n",
    "\n",
    "dataframe = df_payments\n",
    "table_name = 'dim_payments'\n",
    "primary_key = 'payment_key'\n",
    "db_operation = \"update\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8991fea4-970c-431f-afac-a6beb2a97821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data for dimensions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd388fba-55da-4cdb-8165-356e9a198a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerNumber</th>\n",
       "      <th>customerName</th>\n",
       "      <th>contactLastName</th>\n",
       "      <th>contactFirstName</th>\n",
       "      <th>phone</th>\n",
       "      <th>addressLine1</th>\n",
       "      <th>addressLine2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>country</th>\n",
       "      <th>salesRepEmployeeNumber</th>\n",
       "      <th>creditLimit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>Atelier graphique</td>\n",
       "      <td>Schmitt</td>\n",
       "      <td>Carine</td>\n",
       "      <td>40.32.2555</td>\n",
       "      <td>54, rue Royale</td>\n",
       "      <td>None</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>None</td>\n",
       "      <td>44000</td>\n",
       "      <td>France</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>Signal Gift Stores</td>\n",
       "      <td>King</td>\n",
       "      <td>Jean</td>\n",
       "      <td>7025551838</td>\n",
       "      <td>8489 Strong St.</td>\n",
       "      <td>None</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>83030</td>\n",
       "      <td>USA</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>71800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerNumber        customerName contactLastName contactFirstName  \\\n",
       "0             103   Atelier graphique         Schmitt          Carine    \n",
       "1             112  Signal Gift Stores            King             Jean   \n",
       "\n",
       "        phone     addressLine1 addressLine2       city state postalCode  \\\n",
       "0  40.32.2555   54, rue Royale         None     Nantes  None      44000   \n",
       "1  7025551838  8489 Strong St.         None  Las Vegas    NV      83030   \n",
       "\n",
       "  country  salesRepEmployeeNumber  creditLimit  \n",
       "0  France                  1370.0      21000.0  \n",
       "1     USA                  1166.0      71800.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching data from customers\n",
    "sql_customers = \"SELECT * FROM classicmodels.customers;\"\n",
    "df_customers = get_dataframe(user_id, pwd, host_name, src_dbname, sql_customers)\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4df2232c-1c8c-43e0-ba3f-14aeb4d33419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>RequiredDate</th>\n",
       "      <th>ShippedDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>Comments</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10100</td>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>2003-01-10</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>None</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10101</td>\n",
       "      <td>2003-01-09</td>\n",
       "      <td>2003-01-18</td>\n",
       "      <td>2003-01-11</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>Check on availability.</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrderID   OrderDate RequiredDate ShippedDate   Status  \\\n",
       "0    10100  2003-01-06   2003-01-13  2003-01-10  Shipped   \n",
       "1    10101  2003-01-09   2003-01-18  2003-01-11  Shipped   \n",
       "\n",
       "                 Comments  CustomerID  \n",
       "0                    None         363  \n",
       "1  Check on availability.         128  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching data from orders\n",
    "sql_orders = \"SELECT * FROM classicmodels_dw.orders\"\n",
    "df_orders = get_dataframe(user_id, pwd, host_name, dst_dbname, sql_orders)\n",
    "df_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef5e7b0a-9b14-4d91-b800-94f36af502c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productCode</th>\n",
       "      <th>productName</th>\n",
       "      <th>productLine</th>\n",
       "      <th>productScale</th>\n",
       "      <th>productVendor</th>\n",
       "      <th>productDescription</th>\n",
       "      <th>quantityInStock</th>\n",
       "      <th>buyPrice</th>\n",
       "      <th>MSRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S10_1678</td>\n",
       "      <td>1969 Harley Davidson Ultimate Chopper</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Min Lin Diecast</td>\n",
       "      <td>This replica features working kickstand, front...</td>\n",
       "      <td>7933</td>\n",
       "      <td>48.81</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S10_1949</td>\n",
       "      <td>1952 Alpine Renault 1300</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Classic Metal Creations</td>\n",
       "      <td>Turnable front wheels; steering function; deta...</td>\n",
       "      <td>7305</td>\n",
       "      <td>98.58</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  productCode                            productName   productLine  \\\n",
       "0    S10_1678  1969 Harley Davidson Ultimate Chopper   Motorcycles   \n",
       "1    S10_1949               1952 Alpine Renault 1300  Classic Cars   \n",
       "\n",
       "  productScale            productVendor  \\\n",
       "0         1:10          Min Lin Diecast   \n",
       "1         1:10  Classic Metal Creations   \n",
       "\n",
       "                                  productDescription  quantityInStock  \\\n",
       "0  This replica features working kickstand, front...             7933   \n",
       "1  Turnable front wheels; steering function; deta...             7305   \n",
       "\n",
       "   buyPrice   MSRP  \n",
       "0     48.81   95.7  \n",
       "1     98.58  214.3  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching data from products\n",
    "sql_products = \"SELECT * FROM classicmodels.products;\"\n",
    "df_products = get_dataframe(user_id, pwd, host_name, src_dbname, sql_products)\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34a641cc-9072-43c5-af88-262f0c189d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform any necessary transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c04b7622-7417-4833-b6df-f13eb00d60c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customerName</th>\n",
       "      <th>addressLine1</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>Atelier graphique</td>\n",
       "      <td>54, rue Royale</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>None</td>\n",
       "      <td>44000</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>Signal Gift Stores</td>\n",
       "      <td>8489 Strong St.</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>83030</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id        customerName     addressLine1       city  \\\n",
       "0             1          103   Atelier graphique   54, rue Royale     Nantes   \n",
       "1             2          112  Signal Gift Stores  8489 Strong St.  Las Vegas   \n",
       "\n",
       "  state postalCode country  \n",
       "0  None      44000  France  \n",
       "1    NV      83030     USA  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUSTOMERS\n",
    "# 1. Create a List that enumerates the names of each column you wish to remove (drop) from the Pandas DataFrame\n",
    "drop_cols = ['contactLastName','contactFirstName','phone','addressLine2','salesRepEmployeeNumber','creditLimit']\n",
    "df_customers.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Rename the \"customerNumber\" column to \"customer_id\" to reflect the entity as it will serve as the business key for lookup operations\n",
    "df_customers.rename(columns={\"customerNumber\":\"customer_id\"}, inplace=True)\n",
    "\n",
    "# 3. Insert a new column, with an ever-incrementing numeric value, to serve as the primary key.\n",
    "df_customers.insert(0, \"customer_key\", range(1, df_customers.shape[0]+1))\n",
    "\n",
    "# 4. Display the first 2 rows of the dataframe to validate your work\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63289b53-af20-4c46-8975-db78d316cbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>productName</th>\n",
       "      <th>productLine</th>\n",
       "      <th>buyPrice</th>\n",
       "      <th>MSRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>S10_1678</td>\n",
       "      <td>1969 Harley Davidson Ultimate Chopper</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>48.81</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>S10_1949</td>\n",
       "      <td>1952 Alpine Renault 1300</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>98.58</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key product_id                            productName  \\\n",
       "0            1   S10_1678  1969 Harley Davidson Ultimate Chopper   \n",
       "1            2   S10_1949               1952 Alpine Renault 1300   \n",
       "\n",
       "    productLine  buyPrice   MSRP  \n",
       "0   Motorcycles     48.81   95.7  \n",
       "1  Classic Cars     98.58  214.3  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRODUCTS\n",
    "# 1. Create a List that enumerates the names of each column you wish to remove (drop) from the Pandas DataFrame\n",
    "drop_cols = ['productScale','productVendor','productDescription','quantityInStock']\n",
    "df_products.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Rename the \"productCode\" column to \"product_id\" to reflect the entity as it will serve as the business key for lookup operations\n",
    "df_products.rename(columns={\"productCode\":\"product_id\"}, inplace=True)\n",
    "\n",
    "# 3. Insert a new column, with an ever-incrementing numeric value, to serve as the primary key.\n",
    "df_products.insert(0, \"product_key\", range(1, df_products.shape[0]+1))\n",
    "\n",
    "# 4. Display the first 2 rows of the dataframe to validate your work\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d4d0bc2-e48e-4816-95b9-463fff72ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tables_in_classicmodels_dw\n",
      "0              dim_customers\n",
      "1                   dim_date\n",
      "2               dim_payments\n",
      "3               dim_products\n",
      "4                     orders\n",
      "5                 sales_fact\n"
     ]
    }
   ],
   "source": [
    "sql_check_table = \"SHOW TABLES;\"\n",
    "df_tables = get_dataframe(user_id, pwd, host_name, dst_dbname, sql_check_table)\n",
    "print(df_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "018f7daa-2fa0-4412-a238-fa8f8756e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Transformed DataFrames into the New Data Warehouse by Creating New Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1055228-d48d-430e-acd9-11502d645ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_operation = \"update\"\n",
    "\n",
    "tables = [('dim_customers', df_customers, 'customer_key'),\n",
    "          ('dim_products', df_products, 'product_key')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "056766c4-8648-4b2c-93e0-526fa07ccc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into dim_customers with primary key customer_key\n",
      "Inserting into dim_products with primary key product_key\n"
     ]
    }
   ],
   "source": [
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97e41c46-4983-47e0-aaa5-cea080b53b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and populating the fact table called sales_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46fca70e-3f4e-4d8c-b981-ce1946d0dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tables_in_classicmodels_dw\n",
      "0              dim_customers\n",
      "1                   dim_date\n",
      "2               dim_payments\n",
      "3               dim_products\n",
      "4                     orders\n",
      "5                 sales_fact\n"
     ]
    }
   ],
   "source": [
    "sql_check_table = \"SHOW TABLES;\"\n",
    "df_tables = get_dataframe(user_id, pwd, host_name, dst_dbname, sql_check_table)\n",
    "print(df_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afb14f0d-a021-4ddb-a9da-ee31b176a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 4)\n",
      "   customer_key  payment_key  total_price  paymentDate_key\n",
      "0           103            1      6066.78         20041019\n",
      "1           103            2     14571.44         20030605\n"
     ]
    }
   ],
   "source": [
    "sql_sales_fact = \"\"\"\n",
    "SELECT \n",
    "    c.customer_key, \n",
    "    py.payment_key, \n",
    "    py.amount AS total_price,\n",
    "    py.paymentDate_key\n",
    "FROM dim_payments py\n",
    "JOIN dim_customers c ON py.customerNumber = c.customer_key\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_sales_fact = get_dataframe(user_id, pwd, host_name, dst_dbname, sql_sales_fact)\n",
    "print(df_sales_fact.shape)  # Should be a non-zero shape if data exists\n",
    "print(df_sales_fact.head(2))  # Check the first 2 rows to ensure data is fetched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2e07042-65ea-40c6-9606-cd0d2eb128c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_fact_key</th>\n",
       "      <th>customer_key</th>\n",
       "      <th>payment_key</th>\n",
       "      <th>total_price</th>\n",
       "      <th>paymentDate_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>6066.78</td>\n",
       "      <td>20041019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>14571.44</td>\n",
       "      <td>20030605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_fact_key  customer_key  payment_key  total_price  paymentDate_key\n",
       "0               1           103            1      6066.78         20041019\n",
       "1               2           103            2     14571.44         20030605"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Insert a new column, with an ever-incrementing numeric value, to serve as the primary key.\n",
    "df_sales_fact.insert(0, \"sales_fact_key\", range(1, df_sales_fact.shape[0] + 1))\n",
    "# df_sales_fact.drop(columns=[\"sales_fact_key\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# 4. Display the first 2 rows of the dataframe to validate your work\n",
    "df_sales_fact.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa6fa645-5882-4d8b-824e-8cf9f26c3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into the sales_fact table.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_fact_key</th>\n",
       "      <th>customer_key</th>\n",
       "      <th>payment_key</th>\n",
       "      <th>total_price</th>\n",
       "      <th>paymentDate_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>6066.78</td>\n",
       "      <td>20041019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>14571.44</td>\n",
       "      <td>20030605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>1676.14</td>\n",
       "      <td>20041218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>14191.12</td>\n",
       "      <td>20041217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>32641.98</td>\n",
       "      <td>20030606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_fact_key  customer_key  payment_key  total_price  paymentDate_key\n",
       "0               1           103            1      6066.78         20041019\n",
       "1               2           103            2     14571.44         20030605\n",
       "2               3           103            3      1676.14         20041218\n",
       "3               4           112            4     14191.12         20041217\n",
       "4               5           112            5     32641.98         20030606"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data into the sales_fact table\n",
    "\n",
    "df_sales_fact.to_sql('sales_fact', con=sqlEngine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "print(\"Data successfully inserted into the sales_fact table.\")\n",
    "df_sales_fact.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26695a33-ca80-4b8e-abd7-165a5fbeb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate that the new data warehouse exists and contains the correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a401fc0-eb49-4dbe-be3a-53a1b7a3ed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70d6f1eb-ad46-427b-bbb5-e89e11f97215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_key              customerName  total_sales  total_revenue  \\\n",
      "0           114       Mit Vergnügen & Co.            4      180585.07   \n",
      "1           119  Signal Collectibles Ltd.            3      116949.68   \n",
      "\n",
      "   avg_payment_amount  \n",
      "0        45146.267500  \n",
      "1        38983.226667  \n"
     ]
    }
   ],
   "source": [
    "#This query retrieves total sales data for each customer, including total transactions, revenue, and average payment amount.\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_key, \n",
    "    c.customerName, \n",
    "    COUNT(sf.sales_fact_key) AS total_sales, \n",
    "    SUM(py.amount) AS total_revenue, \n",
    "    AVG(py.amount) AS avg_payment_amount\n",
    "FROM sales_fact sf\n",
    "JOIN dim_payments py ON sf.payment_key = py.payment_key\n",
    "JOIN dim_customers c ON py.customerNumber = c.customer_key\n",
    "GROUP BY c.customer_key, c.customerName\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query and load the results into a Pandas DataFrame\n",
    "df_total_sales_by_customer = pd.read_sql(sql_query, sqlEngine)\n",
    "\n",
    "# Display the first 2 rows to check the output\n",
    "print(df_total_sales_by_customer.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8978c3f2-b5e1-43a9-9977-a9076dbfe3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of midterm, start of captstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ea4a5b6-cd40-41c2-8c05-ebb35d0d1ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: parse error near `-m'\n",
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.12/site-packages (4.11.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (2024.12.14)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: findspark in /opt/anaconda3/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pymongo\n",
    "!pip install pymongo certifi pandas findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af47c246-6735-47ee-acbe-fb2a131b20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pyspark\n",
      "/Users/aryaadeshpande/Desktop\n",
      "['Screenshot 2025-05-04 at 7.07.26\\u202fPM.png', 'sqljdbc_12.10.0.0_enu.zip', 'Visual Studio Code.app', 'Screenshot 2025-05-07 at 10.54.04\\u202fPM.png', 'Lab_4_Unsupervised_Learning.pdf', 'orders.csv', '.DS_Store', 'DS 2002', 'Lab_02c_Create_Populate_Dim_Date (1).sql', 'lab_data', '[DS2002] 1Midterm.html', 'Design Portfolio Link.pdf', 'Lab-06-PySpark-Data-Lakehouse-Structured-Streaming.ipynb', '6.0-Lab-DataLakehouse-Structured-Streaming.ipynb', '.localized', 'Screenshot 2025-04-21 at 3.38.30\\u202fPM.png', 'DS2002_Final_Project (1) (1).ipynb', 'RESUME - Aryaa Deshpande (8).pdf', 'DS-2002-main', 'mysql-connector-j-9.1.0', 'mysql-connector-j-9.1.0.jar', 'classic_model_products.json', 'Demo-05-PySpark-Database-Connectivity.ipynb', 'DS2002_Final_Project.ipynb', 'DS2002_Final_Project (1) (3).ipynb', 'mysql-connector-j-9.3.0.jar', 'DS2002_Final_Project (1).ipynb', 'DS 3001', 'classic_orders_2c.json', 'spark-warehouse', 'DS-2002-Data-Project-1.pdf', 'mysql-connector-j-9.3.0.zip', 'DS-2002-Data-Project-2-Capstone.pdf', 'products.json', '.ipynb_checkpoints', 'classic_models_products.json', 'northwind_shippers.csv', 'Screenshot 2025-05-07 at 10.55.09\\u202fPM.png', 'Transcript_UVA01_3019315.pdf', '06-Python-MongoDB.ipynb', 'data', 'sqljdbc_12.8', 'mysql-connector-j-9.3.0', 'Minds and Machines - Exam Study Guide (Spring 2025).docx', 'classic_orders_3c.json', 'Screenshot 2025-04-29 at 6.17.36\\u202fPM.png']\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4fcfb7c-4377-474d-b82b-127d37cbbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"aryaa_desh\",\n",
    "    \"password\" : \"Strawberrylime\",\n",
    "    \"cluster_name\" : \"Cluster0\",\n",
    "    \"cluster_subnet\" : \"xxxxx\",\n",
    "    \"cluster_location\" : \"local\", # \"local\"\n",
    "    \"db_name\" : \"classicmodels_dw\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'classic_models')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'customers')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"classic_models_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "customers_output_bronze = os.path.join(database_dir, 'dim_customers', 'bronze')\n",
    "customers_output_silver = os.path.join(database_dir, 'dim_customers', 'silver')\n",
    "customers_output_gold = os.path.join(database_dir, 'dim_customers', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aff6c6b4-7f96-46cd-a1e9-dbb7ed2cd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Global Functions\n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark C Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = [json.loads(line) for line in openfile if line.strip()]\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "\n",
    "mongodb_args[\"null_column_threshold\"] = 0.5  # or another threshold value\n",
    "\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d840d1fb-0057-421d-9be2-5dfc8ba53842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26c179e1-1bac-4157-a27a-9c02af803e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Lakehouse Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7fec0222-7639-427e-ad21-ffa77ae0c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the data lakehouse database directory structure to ensure idempotency\n",
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d97d149a-d9da-4b16-85b2-efe47b95ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 00:12:06 WARN Utils: Your hostname, Aryaas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.41.1.51 instead (on interface en0)\n",
      "25/05/08 00:12:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/aryaadeshpande/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/aryaadeshpande/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2572d880-d8c7-4526-8b2f-6148be106ad4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 298ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2572d880-d8c7-4526-8b2f-6148be106ad4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n",
      "25/05/08 00:12:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.41.1.51:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark C Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1248a74a0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create A New Spark Session\n",
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = \"/Users/aryaadeshpande/Desktop/mysql-connector-j-9.1.0.jar\"\n",
    "# mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cd791-7eb9-4f54-9c1f-1073fe461761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e65bab9-5ea2-4ac7-b5ff-811dbb395c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a New Metadata Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6af076e-3477-482c-94a4-2ed761690794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryaadeshpande/Desktop/mysql-connector-j-9.1.0.jar\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))\n",
    "import os\n",
    "mysql_jar_path = \"/Users/aryaadeshpande/Desktop/mysql-connector-j-9.1.0.jar\"\n",
    "print(os.path.exists(mysql_jar_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fd9a10d-547d-46a9-8861-c31f88215321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'Capstone Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Capstone');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50ea5db0-a689-4d6c-9d93-2dac8e3d3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "#Fetch data from the file system \n",
    "#Verify the location of the source data files on the file system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c4db7f4b-00bd-403c-96bd-70c0bb319e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.DS_Store</td>\n",
       "      <td>6148</td>\n",
       "      <td>2025-05-08 04:04:11.139058828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic_models_customers.csv</td>\n",
       "      <td>5473</td>\n",
       "      <td>2025-05-06 21:07:13.551294327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_models_employees.csv</td>\n",
       "      <td>1781</td>\n",
       "      <td>2025-05-06 21:03:53.490443945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classic_models_orders.csv</td>\n",
       "      <td>3389</td>\n",
       "      <td>2025-05-06 21:09:11.481813908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classic_models_productlines.csv</td>\n",
       "      <td>3446</td>\n",
       "      <td>2025-05-06 20:57:26.006496668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>classic_models_productlines.json</td>\n",
       "      <td>3886</td>\n",
       "      <td>2025-05-07 22:06:31.654823780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>classic_models_products.csv</td>\n",
       "      <td>2011</td>\n",
       "      <td>2025-05-07 01:56:58.468213081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>classic_models_products.json</td>\n",
       "      <td>4686</td>\n",
       "      <td>2025-05-08 03:16:45.490648985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  size             modification_time\n",
       "0                         .DS_Store  6148 2025-05-08 04:04:11.139058828\n",
       "1      classic_models_customers.csv  5473 2025-05-06 21:07:13.551294327\n",
       "2      classic_models_employees.csv  1781 2025-05-06 21:03:53.490443945\n",
       "3         classic_models_orders.csv  3389 2025-05-06 21:09:11.481813908\n",
       "4   classic_models_productlines.csv  3446 2025-05-06 20:57:26.006496668\n",
       "5  classic_models_productlines.json  3886 2025-05-07 22:06:31.654823780\n",
       "6       classic_models_products.csv  2011 2025-05-07 01:56:58.468213081\n",
       "7      classic_models_products.json  4686 2025-05-08 03:16:45.490648985"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e786a272-89b2-4a79-bdef-020cf58928c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryaadeshpande/Desktop/lab_data/classic_models/batch/classic_models_employees.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeNumber</th>\n",
       "      <th>lastName</th>\n",
       "      <th>firstName</th>\n",
       "      <th>extension</th>\n",
       "      <th>email</th>\n",
       "      <th>officeCode</th>\n",
       "      <th>reportsTo</th>\n",
       "      <th>jobTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002</td>\n",
       "      <td>Murphy</td>\n",
       "      <td>Diane</td>\n",
       "      <td>x5800</td>\n",
       "      <td>dmurphy@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>President</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056</td>\n",
       "      <td>Patterson</td>\n",
       "      <td>Mary</td>\n",
       "      <td>x4611</td>\n",
       "      <td>mpatterso@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>VP Sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employeeNumber   lastName firstName extension  \\\n",
       "0            1002     Murphy     Diane     x5800   \n",
       "1            1056  Patterson      Mary     x4611   \n",
       "\n",
       "                            email  officeCode  reportsTo   jobTitle  \n",
       "0    dmurphy@classicmodelcars.com           1        NaN  President  \n",
       "1  mpatterso@classicmodelcars.com           1     1002.0   VP Sales  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate employees dimension \n",
    "# Use pyspark to read from a CSV file \n",
    "employee_csv = os.path.join(batch_dir, 'classic_models_employees.csv')\n",
    "print(employee_csv)\n",
    "\n",
    "df_dim_employees = spark.read.format('csv').options(header='true', inferSchema='true').load(employee_csv)\n",
    "df_dim_employees.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aad906ea-47f0-427b-9021-ed34e6222a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>lastName</th>\n",
       "      <th>firstName</th>\n",
       "      <th>extension</th>\n",
       "      <th>email</th>\n",
       "      <th>officeCode</th>\n",
       "      <th>reportsTo</th>\n",
       "      <th>jobTitle</th>\n",
       "      <th>employee_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002</td>\n",
       "      <td>Murphy</td>\n",
       "      <td>Diane</td>\n",
       "      <td>x5800</td>\n",
       "      <td>dmurphy@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>President</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056</td>\n",
       "      <td>Patterson</td>\n",
       "      <td>Mary</td>\n",
       "      <td>x4611</td>\n",
       "      <td>mpatterso@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>VP Sales</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id   lastName firstName extension                           email  \\\n",
       "0         1002     Murphy     Diane     x5800    dmurphy@classicmodelcars.com   \n",
       "1         1056  Patterson      Mary     x4611  mpatterso@classicmodelcars.com   \n",
       "\n",
       "   officeCode  reportsTo   jobTitle  employee_key  \n",
       "0           1        NaN  President             1  \n",
       "1           1     1002.0   VP Sales             2  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make neccessary transformations to employees dataframe\n",
    "# rename 'id' column to 'employee_id'\n",
    "df_dim_employees = df_dim_employees.withColumnRenamed(\"employeeNumber\", \"employee_id\")\n",
    "# Add primary key column \n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_employees = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY employee_id) AS employee_key\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_employees)\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67d27109-03f9-45c1-ab8b-971952e20d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# save as the dim_employees in the data lakehouse\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db/dim_employees\", ignore_errors=True)\n",
    "\n",
    "\n",
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5cef8b78-6f01-4773-83f0-4f27ad7d6293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         employee_id|                 int|   NULL|\n",
      "|            lastName|              string|   NULL|\n",
      "|           firstName|              string|   NULL|\n",
      "|           extension|              string|   NULL|\n",
      "|               email|              string|   NULL|\n",
      "|          officeCode|                 int|   NULL|\n",
      "|           reportsTo|              double|   NULL|\n",
      "|            jobTitle|              string|   NULL|\n",
      "|        employee_key|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  classic_models_dlh|       |\n",
      "|               Table|       dim_employees|       |\n",
      "|        Created Time|Thu May 08 00:12:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/aryaa...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>lastName</th>\n",
       "      <th>firstName</th>\n",
       "      <th>extension</th>\n",
       "      <th>email</th>\n",
       "      <th>officeCode</th>\n",
       "      <th>reportsTo</th>\n",
       "      <th>jobTitle</th>\n",
       "      <th>employee_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002</td>\n",
       "      <td>Murphy</td>\n",
       "      <td>Diane</td>\n",
       "      <td>x5800</td>\n",
       "      <td>dmurphy@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>President</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056</td>\n",
       "      <td>Patterson</td>\n",
       "      <td>Mary</td>\n",
       "      <td>x4611</td>\n",
       "      <td>mpatterso@classicmodelcars.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>VP Sales</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id   lastName firstName extension                           email  \\\n",
       "0         1002     Murphy     Diane     x5800    dmurphy@classicmodelcars.com   \n",
       "1         1056  Patterson      Mary     x4611  mpatterso@classicmodelcars.com   \n",
       "\n",
       "   officeCode  reportsTo   jobTitle  employee_key  \n",
       "0           1        NaN  President             1  \n",
       "1           1     1002.0   VP Sales             2  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test: describe and preview the table\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a0ef249-b463-486b-9e96-bb4549f456d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryaadeshpande/Desktop/lab_data/classic_models/batch/classic_models_productlines.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productLine</th>\n",
       "      <th>textDescription</th>\n",
       "      <th>htmlDescription</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>Attention car enthusiasts: Make your wildest c...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>Our motorcycles are state of the art replicas ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productLine                                    textDescription  \\\n",
       "0  Classic Cars  Attention car enthusiasts: Make your wildest c...   \n",
       "1   Motorcycles  Our motorcycles are state of the art replicas ...   \n",
       "\n",
       "  htmlDescription image  \n",
       "0            None  None  \n",
       "1            None  None  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate productlines dimension\n",
    "# Use pyspark to read from a CSV file \n",
    "productlines_csv = os.path.join(batch_dir, 'classic_models_productlines.csv')\n",
    "print(productlines_csv)\n",
    "\n",
    "df_dim_productlines = spark.read.format('csv').options(header='true', inferSchema='true').load(productlines_csv)\n",
    "df_dim_productlines.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d47558a-5e36-469f-b3db-a41d214cf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as the dim_productlines in the data lakehouse\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db/dim_productlines\", ignore_errors=True)\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS classic_models_dlh.dim_productlines\")\n",
    "df_dim_productlines.write.saveAsTable(f\"{dest_database}.dim_productlines\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4866f527-ba89-429b-a85f-c84d9e8d0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryaadeshpande/Desktop/lab_data/classic_models/batch/classic_models_customers.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerNumber</th>\n",
       "      <th>customerName</th>\n",
       "      <th>contactLastName</th>\n",
       "      <th>contactFirstName</th>\n",
       "      <th>phone</th>\n",
       "      <th>addressLine1</th>\n",
       "      <th>addressLine2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>country</th>\n",
       "      <th>salesRepEmployeeNumber</th>\n",
       "      <th>creditLimit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>Atelier graphique</td>\n",
       "      <td>Schmitt</td>\n",
       "      <td>Carine</td>\n",
       "      <td>40.32.2555</td>\n",
       "      <td>54, rue Royale</td>\n",
       "      <td>None</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>None</td>\n",
       "      <td>44000</td>\n",
       "      <td>France</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>Signal Gift Stores</td>\n",
       "      <td>King</td>\n",
       "      <td>Jean</td>\n",
       "      <td>7025551838</td>\n",
       "      <td>8489 Strong St.</td>\n",
       "      <td>None</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>83030</td>\n",
       "      <td>USA</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>71800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerNumber        customerName contactLastName contactFirstName  \\\n",
       "0             103   Atelier graphique         Schmitt           Carine   \n",
       "1             112  Signal Gift Stores            King             Jean   \n",
       "\n",
       "        phone     addressLine1 addressLine2       city state postalCode  \\\n",
       "0  40.32.2555   54, rue Royale         None     Nantes  None      44000   \n",
       "1  7025551838  8489 Strong St.         None  Las Vegas    NV      83030   \n",
       "\n",
       "  country  salesRepEmployeeNumber  creditLimit  \n",
       "0  France                  1370.0      21000.0  \n",
       "1     USA                  1166.0      71800.0  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate customers dimension \n",
    "# Use pyspark to read from a CSV file \n",
    "customers_csv = os.path.join(batch_dir, 'classic_models_customers.csv')\n",
    "print(customers_csv)\n",
    "\n",
    "df_dim_customers = spark.read.format('csv').options(header='true', inferSchema='true').load(customers_csv)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "92f8730b-5368-4d2b-ba08-3cef62bb1c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customerName</th>\n",
       "      <th>contactLastName</th>\n",
       "      <th>contactFirstName</th>\n",
       "      <th>phone</th>\n",
       "      <th>addressLine1</th>\n",
       "      <th>addressLine2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>country</th>\n",
       "      <th>salesRepEmployeeNumber</th>\n",
       "      <th>creditLimit</th>\n",
       "      <th>customer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>Atelier graphique</td>\n",
       "      <td>Schmitt</td>\n",
       "      <td>Carine</td>\n",
       "      <td>40.32.2555</td>\n",
       "      <td>54, rue Royale</td>\n",
       "      <td>None</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>None</td>\n",
       "      <td>44000</td>\n",
       "      <td>France</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>Signal Gift Stores</td>\n",
       "      <td>King</td>\n",
       "      <td>Jean</td>\n",
       "      <td>7025551838</td>\n",
       "      <td>8489 Strong St.</td>\n",
       "      <td>None</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>83030</td>\n",
       "      <td>USA</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>71800.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id        customerName contactLastName contactFirstName  \\\n",
       "0          103   Atelier graphique         Schmitt           Carine   \n",
       "1          112  Signal Gift Stores            King             Jean   \n",
       "\n",
       "        phone     addressLine1 addressLine2       city state postalCode  \\\n",
       "0  40.32.2555   54, rue Royale         None     Nantes  None      44000   \n",
       "1  7025551838  8489 Strong St.         None  Las Vegas    NV      83030   \n",
       "\n",
       "  country  salesRepEmployeeNumber  creditLimit  customer_key  \n",
       "0  France                  1370.0      21000.0             1  \n",
       "1     USA                  1166.0      71800.0             2  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make neccessary transformations to customers dataframe\n",
    "# rename 'customerNumber' column to 'customer_id'\n",
    "customers_csv = os.path.join(batch_dir, 'classic_models_customers.csv')\n",
    "df_dim_customers = spark.read.format('csv').options(header='true', inferSchema=True).load(customers_csv)\n",
    "\n",
    "df_dim_customers = df_dim_customers.withColumnRenamed(\"customerNumber\", \"customer_id\")\n",
    "# Add order primary key column \n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY customer_id) AS customer_key\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customer = spark.sql(sql_customers)\n",
    "df_dim_customer.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20686b1d-cd7a-4b78-b6d5-b10e2183325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as the dim_customers in the data lakehouse\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db/dim_customers\", ignore_errors=True)\n",
    "\n",
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7120345b-b360-4609-881a-40745e031458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the Date Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40acf0f9-8f2b-4042-b805-84f163aa5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from dim_date table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "95f21387-bc7f-4358-8eba-c0051ba725b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/Users/aryaadeshpande/Desktop/mysql-connector-j-9.1.0.jar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"mysql-connector-j-9.1.0.jar\"))\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "946f9a61-d0be-46fc-8f36-e8e77cd7048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_args = {\n",
    "    \"host_name\": \"localhost\",         # or the remote host address\n",
    "    \"port\": \"3306\",\n",
    "    \"db_name\": \"classicmodels_dw\",  # must match the key used in get_mysql_dataframe\n",
    "    \"conn_props\": {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"Strawberrylime\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4697e0a5-db62-457e-80ed-f022699dcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|   TABLE_NAME|\n",
      "+-------------+\n",
      "|dim_customers|\n",
      "|     dim_date|\n",
      "| dim_payments|\n",
      "| dim_products|\n",
      "|       orders|\n",
      "|   sales_fact|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_test = f\"\"\"\n",
    "SELECT TABLE_NAME \n",
    "FROM INFORMATION_SCHEMA.TABLES \n",
    "WHERE TABLE_SCHEMA = '{mysql_args['db_name']}'\n",
    "\"\"\"\n",
    "df_test = get_mysql_dataframe(spark, sql_test, **mysql_args)\n",
    "df_test.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a057a654-f5be-42ab-bf96-7ca23d32b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "511708c3-694d-45f0-9e73-1e2dc2c0a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save dim_date table in data lakehouse\n",
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "758faa32-cba3-49ff-99ad-a237815b1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch Reference Data from a MongoDB Atlas Database\n",
    "####Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "afdd45d2-0708-4024-bb76-1ba14a51f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "622951db-8189-472f-be7b-1a17da5a4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"products\" : \"classic_models_products.json\"}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9eed7e73-4257-4bc4-a42a-b64ac5188b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSRP</th>\n",
       "      <th>buyPrice</th>\n",
       "      <th>productCode</th>\n",
       "      <th>productDescription</th>\n",
       "      <th>productLine</th>\n",
       "      <th>productName</th>\n",
       "      <th>productScale</th>\n",
       "      <th>productVendor</th>\n",
       "      <th>quantityInStock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.7</td>\n",
       "      <td>48.81</td>\n",
       "      <td>S10_1678</td>\n",
       "      <td>This replica features working kickstand, front...</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>1969 Harley Davidson Ultimate Chopper</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Min Lin Diecast</td>\n",
       "      <td>7933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214.3</td>\n",
       "      <td>98.58</td>\n",
       "      <td>S10_1949</td>\n",
       "      <td>Turnable front wheels; steering function; deta...</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1952 Alpine Renault 1300</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Classic Metal Creations</td>\n",
       "      <td>7305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSRP  buyPrice productCode  \\\n",
       "0   95.7     48.81    S10_1678   \n",
       "1  214.3     98.58    S10_1949   \n",
       "\n",
       "                                  productDescription   productLine  \\\n",
       "0  This replica features working kickstand, front...   Motorcycles   \n",
       "1  Turnable front wheels; steering function; deta...  Classic Cars   \n",
       "\n",
       "                             productName productScale  \\\n",
       "0  1969 Harley Davidson Ultimate Chopper         1:10   \n",
       "1               1952 Alpine Renault 1300         1:10   \n",
       "\n",
       "             productVendor  quantityInStock  \n",
       "0          Min Lin Diecast             7933  \n",
       "1  Classic Metal Creations             7305  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Populate the products Dimension \n",
    "##### 2.2.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Customers</span> Collection\n",
    "mongodb_args[\"collection\"] = \"products\"\n",
    "mongodb_args[\"null_column_threshold\"] = 0.5\n",
    "\n",
    "df_dim_products = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3206f9e7-3ca6-4418-8fa9-c2f25ad6c5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MSRP: double (nullable = true)\n",
      " |-- buyPrice: double (nullable = true)\n",
      " |-- productCode: string (nullable = true)\n",
      " |-- productDescription: string (nullable = true)\n",
      " |-- productLine: string (nullable = true)\n",
      " |-- productName: string (nullable = true)\n",
      " |-- productScale: string (nullable = true)\n",
      " |-- productVendor: string (nullable = true)\n",
      " |-- quantityInStock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "69bec72f-e4b0-436c-8751-b70e9fec1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>productName</th>\n",
       "      <th>productLine</th>\n",
       "      <th>productDescription</th>\n",
       "      <th>productScale</th>\n",
       "      <th>productVendor</th>\n",
       "      <th>quantityInStock</th>\n",
       "      <th>buyPrice</th>\n",
       "      <th>MSRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>S10_1678</td>\n",
       "      <td>1969 Harley Davidson Ultimate Chopper</td>\n",
       "      <td>Motorcycles</td>\n",
       "      <td>This replica features working kickstand, front...</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Min Lin Diecast</td>\n",
       "      <td>7933</td>\n",
       "      <td>48.81</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>S10_1949</td>\n",
       "      <td>1952 Alpine Renault 1300</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>Turnable front wheels; steering function; deta...</td>\n",
       "      <td>1:10</td>\n",
       "      <td>Classic Metal Creations</td>\n",
       "      <td>7305</td>\n",
       "      <td>98.58</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key product_id                            productName  \\\n",
       "0            1   S10_1678  1969 Harley Davidson Ultimate Chopper   \n",
       "1            2   S10_1949               1952 Alpine Renault 1300   \n",
       "\n",
       "    productLine                                 productDescription  \\\n",
       "0   Motorcycles  This replica features working kickstand, front...   \n",
       "1  Classic Cars  Turnable front wheels; steering function; deta...   \n",
       "\n",
       "  productScale            productVendor  quantityInStock  buyPrice   MSRP  \n",
       "0         1:10          Min Lin Diecast             7933     48.81   95.7  \n",
       "1         1:10  Classic Metal Creations             7305     98.58  214.3  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'productCode' column to 'product_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"productCode\", \"product_id\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "window_spec = Window.orderBy(\"product_id\")\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products = df_dim_products.select(\n",
    "    \"product_key\",\n",
    "    \"product_id\",\n",
    "    \"productName\",\n",
    "    \"productLine\",\n",
    "    \"productDescription\",\n",
    "    \"productScale\",\n",
    "    \"productVendor\",\n",
    "    \"quantityInStock\",\n",
    "    \"buyPrice\",\n",
    "    \"MSRP\"\n",
    ")\n",
    "\n",
    "# Show first 2 rows for verification\n",
    "df_dim_products.toPandas().head(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9e4b8b8-95af-43f2-ab32-286fea23b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as dim_products to data lakehouse\n",
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6149d3ef-f2c8-4782-80ea-9003de73b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test: describe and preview the table\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7d2d3a43-4be9-4886-9d4a-005865562935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classic_models_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic_models_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_models_dlh</td>\n",
       "      <td>dim_employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classic_models_dlh</td>\n",
       "      <td>dim_productlines</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classic_models_dlh</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>employees</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            namespace         tableName  isTemporary\n",
       "0  classic_models_dlh     dim_customers        False\n",
       "1  classic_models_dlh          dim_date        False\n",
       "2  classic_models_dlh     dim_employees        False\n",
       "3  classic_models_dlh  dim_productlines        False\n",
       "4  classic_models_dlh      dim_products        False\n",
       "5                             customers         True\n",
       "6                             employees         True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the dimension tables\n",
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c8d112e7-b2df-4f83-98a1-598a7f10b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#III. Integrate reference data with real time data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "84533f61-ff67-4f97-94a5-149e0395748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/Users/aryaadeshpande/Desktop/lab_data/checkpoints/orders_silver\", ignore_errors=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c1f8001c-cafb-4aac-808b-b720ecd27cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing bronze files\n",
    "import os\n",
    "import glob\n",
    "\n",
    "bronze_dir = \"/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db/fact_orders/bronze\"\n",
    "for file in glob.glob(os.path.join(bronze_dir, \"*.parquet\")):\n",
    "    os.remove(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2d247e40-0eca-4beb-b299-7c5ad06674a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.DS_Store</td>\n",
       "      <td>6148</td>\n",
       "      <td>2025-05-08 04:05:41.836149693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic_orders_1c.json</td>\n",
       "      <td>2068</td>\n",
       "      <td>2025-05-06 00:58:43.275743246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_orders_2c.json</td>\n",
       "      <td>2071</td>\n",
       "      <td>2025-05-08 04:05:16.723586798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classic_orders_3c.json</td>\n",
       "      <td>2155</td>\n",
       "      <td>2025-05-08 04:05:21.069364786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  size             modification_time\n",
       "0               .DS_Store  6148 2025-05-08 04:05:41.836149693\n",
       "1  classic_orders_1c.json  2068 2025-05-06 00:58:43.275743246\n",
       "2  classic_orders_2c.json  2071 2025-05-08 04:05:16.723586798\n",
       "3  classic_orders_3c.json  2155 2025-05-08 04:05:21.069364786"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use PySpark Structured Streaming to Process (Hot Path) to process orders fact data \n",
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3793ffc3-2793-4871-a059-247ccedb803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"orderNumber\", LongType(), True),\n",
    "    StructField(\"orderDate\", StringType(), True),\n",
    "    StructField(\"requiredDate\", StringType(), True),\n",
    "    StructField(\"shippedDate\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"customerNumber\", LongType(), True),\n",
    "    StructField(\"productCode\", StringType(), True),\n",
    "    StructField(\"quantityOrdered\", IntegerType(), True),\n",
    "    StructField(\"priceEach\", DoubleType(), True),\n",
    "    StructField(\"orderLineNumber\", IntegerType(), True),\n",
    "    StructField(\"employeeNumber\", LongType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "56b5acab-dc53-4b5d-a0c5-6cb9387ae4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Bronze Layer: Stage Orders Fact table data \n",
    "# Read \"raw\" JSON file in to a stream\n",
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "    .option(\"maxFilesPerTrigger\", 3)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"basePath\", orders_stream_dir) \\\n",
    "    .schema(orders_schema)\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint_bronze')\n",
    "\n",
    "bronze_query = df_orders_bronze.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"basePath\", orders_output_bronze) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .start(orders_output_bronze)\n",
    "\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5a914136-b5ba-4b2c-885d-2e50a8da3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming data to a parquet file\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint_bronze')\n",
    "\n",
    "\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    # Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6c498244-06db-42c4-bc60-02cb43344671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_checkpoint_bronze', '_spark_metadata']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bronze_path = \"/Users/aryaadeshpande/Desktop/spark-warehouse/classic_models_dlh.db/fact_orders/bronze\"\n",
    "os.listdir(bronze_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ff3512d6-243b-46de-83ec-61703e141d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|month|year|\n",
      "+-----+----+\n",
      "|    1|2023|\n",
      "|    5|2023|\n",
      "|   11|2023|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, month, year\n",
    "\n",
    "df_bronze_check = spark.read.format(\"parquet\").load(orders_output_bronze)\n",
    "\n",
    "# Convert orderDate to proper DateType if needed\n",
    "df_bronze_check = df_bronze_check.withColumn(\"orderDateParsed\", to_date(\"orderDate\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Get distinct months and years\n",
    "df_bronze_check.select(month(\"orderDateParsed\").alias(\"month\"), year(\"orderDateParsed\").alias(\"year\")) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"year\", \"month\") \\\n",
    "    .show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1665ae93-16ba-4b53-949e-fb40f4cc75cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: a74169a4-45c2-4da7-bcb9-6c908af8644d\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Unit Test: Implement Query Monitoring\n",
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a2ae843d-60b7-4250-94be-077fdd37a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bdeb0f4b-f6cc-4fb7-9ce3-db5a48322b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "# Create a pandas range from 2023-01-01 to 2023-12-31\n",
    "start_date = date(2023, 1, 1)\n",
    "end_date = date(2023, 12, 31)\n",
    "date_list = [(start_date + timedelta(days=i)).isoformat() for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "df_dim_date = spark.createDataFrame([(d,) for d in date_list], [\"order_full_date\"]) \\\n",
    "    .withColumn(\"order_full_date\", col(\"order_full_date\").cast(DateType())) \\\n",
    "    .withColumn(\"order_date_key\", expr(\"CAST(date_format(order_full_date, 'yyyyMMdd') AS BIGINT)\"))\n",
    "\n",
    "# Save it to your warehouse\n",
    "df_dim_date.select(\"order_date_key\", \"order_full_date\").write.mode(\"overwrite\").saveAsTable(\"classic_models_dlh.dim_order_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f37c5030-3204-42bd-97a1-4cc9db681278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = spark.table(\"classic_models_dlh.dim_order_date\")\n",
    "\n",
    "# # Confirm your date range now includes 2023\n",
    "# df_dim_order_date.filter(\"full_date = '2023-05-01'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "decdd350-c446-42e0-9e0d-5220c2495ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# No need to alias 'order_date_key' again — it's already correctly named\n",
    "df_dim_order_date = df_dim_date.select(\n",
    "    col(\"order_date_key\"), \n",
    "    col(\"order_full_date\")\n",
    ")\n",
    "df_dim_order_date.write.mode(\"overwrite\").saveAsTable(\"classic_models_dlh.dim_order_date\")\n",
    "\n",
    "# Use correct column names for paid_date and shipped_date\n",
    "df_dim_paid_date = df_dim_date.select(\n",
    "    col(\"order_date_key\").alias(\"paid_date_key\"), \n",
    "    col(\"order_full_date\").alias(\"paid_full_date\")\n",
    ")\n",
    "df_dim_shipped_date = df_dim_date.select(\n",
    "    col(\"order_date_key\").alias(\"shipped_date_key\"), \n",
    "    col(\"order_full_date\").alias(\"shipped_full_date\")\n",
    ")\n",
    "\n",
    "# Reload other dimension tables\n",
    "df_dim_products = spark.table(\"classic_models_dlh.dim_products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "abe5c23f-cae2-4426-9b92-c8dd933808f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderNumber: long (nullable = true)\n",
      " |-- orderDate: string (nullable = true)\n",
      " |-- requiredDate: string (nullable = true)\n",
      " |-- shippedDate: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- customerNumber: long (nullable = true)\n",
      " |-- productCode: string (nullable = true)\n",
      " |-- quantityOrdered: integer (nullable = true)\n",
      " |-- priceEach: double (nullable = true)\n",
      " |-- orderLineNumber: integer (nullable = true)\n",
      " |-- employeeNumber: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca2af04b-cd1e-4741-aae1-14f13933ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, col, to_date\n",
    "from pyspark.sql.types import LongType\n",
    "import os\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 1: Cast join key columns in dimension tables\n",
    "# -----------------------------------------------------\n",
    "df_dim_customers = df_dim_customers.withColumn(\"customer_id\", col(\"customer_id\").cast(LongType()))\n",
    "df_dim_employees = df_dim_employees \\\n",
    "    .withColumn(\"employee_id\", col(\"employee_id\").cast(LongType())) \\\n",
    "    .withColumn(\"employee_key\", col(\"employee_key\").cast(LongType()))\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", col(\"product_key\").cast(LongType()))\n",
    "# df_dim_order_date is already fine (order_full_date is DateType, order_date_key is LongType)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 2: Transform the streaming bronze input\n",
    "# -----------------------------------------------------\n",
    "df_orders_bronze_transformed = df_orders_bronze.withColumn(\n",
    "    \"orderDateParsed\", to_date(col(\"orderDate\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 3: Join with all dimension tables using correct keys\n",
    "# -----------------------------------------------------\n",
    "# Normalize product codes to uppercase for case-insensitive join\n",
    "df_orders_silver = df_orders_bronze_transformed \\\n",
    "    .withColumn(\"productCode_upper\", upper(col(\"productCode\"))) \\\n",
    "    .join(broadcast(df_dim_customers), col(\"customerNumber\") == col(\"customer_id\"), \"left\") \\\n",
    "    .join(broadcast(df_dim_employees), col(\"employeeNumber\") == col(\"employee_id\"), \"left\") \\\n",
    "    .join(\n",
    "        broadcast(df_dim_products.withColumn(\"product_id_upper\", upper(col(\"product_id\")))),\n",
    "        col(\"productCode_upper\") == col(\"product_id_upper\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .join(broadcast(df_dim_order_date), col(\"orderDateParsed\") == col(\"order_full_date\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"orderNumber\").alias(\"order_id\").cast(LongType()),\n",
    "        col(\"orderNumber\").alias(\"order_detail_id\").cast(LongType()),\n",
    "        col(\"customer_id\").cast(LongType()),\n",
    "        col(\"employee_key\").cast(LongType()),\n",
    "        col(\"product_key\").cast(LongType()),\n",
    "        col(\"order_date_key\").cast(LongType()),\n",
    "        col(\"orderDateParsed\"),  \n",
    "        col(\"quantityOrdered\").alias(\"quantity\"),\n",
    "        col(\"priceEach\").alias(\"unit_price\")\n",
    "    ) \\\n",
    "    .filter(col(\"product_key\").isNotNull())  # Still filters invalid products, but now case-insensitively\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 4: Define output and checkpoint directories\n",
    "# -----------------------------------------------------\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, \"_checkpoint_silver\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 5: Write Silver output with streaming trigger\n",
    "# -----------------------------------------------------\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .trigger(availableNow=True)  \n",
    "    .start(orders_output_silver)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0a23d830-b610-436a-9560-e5fadd95f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"basePath\", orders_output_bronze) \\\n",
    "    .load(orders_output_bronze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7a35a07a-5669-4b7d-ab47-eadb96771118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f6f291f9-370b-4ca6-85ca-6e9c446725d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderNumber: long (nullable = true)\n",
      " |-- orderDate: string (nullable = true)\n",
      " |-- requiredDate: string (nullable = true)\n",
      " |-- shippedDate: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- customerNumber: long (nullable = true)\n",
      " |-- productCode: string (nullable = true)\n",
      " |-- quantityOrdered: integer (nullable = true)\n",
      " |-- priceEach: double (nullable = true)\n",
      " |-- orderLineNumber: integer (nullable = true)\n",
      " |-- employeeNumber: long (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4f43d6d1-a624-45d0-983e-299472b00842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+------------+-----------+--------------+---------------+--------+----------+\n",
      "|order_id|order_detail_id|customer_id|employee_key|product_key|order_date_key|orderDateParsed|quantity|unit_price|\n",
      "+--------+---------------+-----------+------------+-----------+--------------+---------------+--------+----------+\n",
      "|10120   |10120          |112        |17          |7          |20231115      |2023-11-15     |9       |291.35    |\n",
      "|10110   |10110          |119        |16          |7          |20230115      |2023-01-15     |7       |144.57    |\n",
      "|10100   |10100          |119        |16          |2          |20230501      |2023-05-01     |2       |53.59     |\n",
      "+--------+---------------+-----------+------------+-----------+--------------+---------------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, None)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver_check = spark.read.format(\"parquet\").load(orders_output_silver)\n",
    "df_silver_check.count(), df_silver_check.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b9b6fe8d-45d1-4c65-bbc5-d69f146e755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Transformed Streaming data to the Data Lakehouse\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(orders_checkpoint_silver, ignore_errors=True)\n",
    "import time\n",
    "\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, f'_checkpoint_{int(time.time())}')\n",
    "\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(orders_output_silver)\n",
    ")\n",
    "\n",
    "# Wait for at least one batch to process\n",
    "\n",
    "while len(orders_silver_query.recentProgress) == 0:\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f4f5f2e6-f424-4ca8-a08d-c42560a931bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(orders_silver_query.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "11df847e-c911-4409-b8c8-f83619d2f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver = spark.readStream.format(\"parquet\").load(orders_output_bronze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bab23ae1-eac9-44df-adb9-95e89ec1bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 57ef4f0d-8368-4e0c-918a-c786fb6091c8\n",
      "Query Name: None\n",
      "Query Status: {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b751238b-3c01-4546-84d3-55676d16b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8dbe7e17-a8c0-4acf-884d-a0d814454b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Layer: Perform Aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a55cb114-fa34-4565-a8ad-f2f0dc28fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Query to Create a Business Report \n",
    "# Create a new Gold table using the Pyspark API. This code reads streaming order data from a Parquet source and joins it with product and date dimension tables to enrich the dataset. It then aggregates the number of products ordered per month and product line, producing a monthly summary sorted by month and product popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1b349d01-edd0-4ab0-9fca-fb80ac090d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, asc, desc, count, month, date_format\n",
    "\n",
    "df_orders_by_product_category_gold = spark.readStream.format(\"parquet\").load(orders_output_silver) \\\n",
    "    .join(df_dim_products, \"product_key\") \\\n",
    "    .withColumn(\"month_of_year\", month(\"orderDateParsed\")) \\\n",
    "    .withColumn(\"month_name\", date_format(\"orderDateParsed\", \"MMMM\")) \\\n",
    "    .groupBy(\"month_name\", \"month_of_year\", \"productLine\") \\\n",
    "    .agg(count(\"*\").alias(\"product_count\")) \\\n",
    "    .orderBy(\"month_of_year\", \"productLine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5eaf3449-f7ef-4507-97fc-da85f8a2d2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "69bdfa51-f7cb-4044-925d-97a81ed2692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- productLine: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_by_product_category_gold.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "06218702-f4ca-48ef-a58a-cd79835c56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming data to a parquet file in \"Complete\" mode\n",
    "# Stop the existing query if it's still running\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"fact_orders_by_product_category\":\n",
    "        q.stop()\n",
    "orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_orders_by_product_category\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "682d545e-9690-443b-81e7-0503cc317073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while len(orders_gold_query.recentProgress) < 1:\n",
    "    time.sleep(5)\n",
    "\n",
    "orders_gold_query.processAllAvailable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "74708aca-3d9a-40b9-a56e-9260463288ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- productLine: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the gold data from memory \n",
    "df_fact_orders_by_product_category = spark.sql(\"SELECT * FROM fact_orders_by_product_category\")\n",
    "df_fact_orders_by_product_category.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "61a329f4-04de-40c5-be5f-2532ff20ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final selection\n",
    "df_fact_orders_by_product_category_gold_final = df_fact_orders_by_product_category \\\n",
    "    .select(\n",
    "        col(\"month_name\").alias(\"Month\"),\n",
    "        col(\"productLine\").alias(\"Product Category\"),\n",
    "        col(\"product_count\").alias(\"Product Count\")\n",
    "    ) \\\n",
    "    .orderBy(asc(\"month_of_year\"), desc(\"Product Count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e7cd928a-b805-4917-a85c-212d7882f5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Product Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>November</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>January</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May</td>\n",
       "      <td>Classic Cars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month Product Category  Product Count\n",
       "0  November     Classic Cars              1\n",
       "1   January     Classic Cars              1\n",
       "2       May     Classic Cars              1"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Final Results into a New Table and Display the Results\n",
    "# Drop the table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {dest_database}.fact_orders_by_product_category\")\n",
    "\n",
    "# Delete the associated directory manually\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "table_path = f\"/Users/aryaadeshpande/Desktop/spark-warehouse/{dest_database}.db/fact_orders_by_product_category\"\n",
    "if os.path.exists(table_path):\n",
    "    shutil.rmtree(table_path)\n",
    "df_fact_orders_by_product_category_gold_final.write.saveAsTable(f\"{dest_database}.fact_orders_by_product_category\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_product_category\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6f0ad-12d2-4aca-b661-f2607c12206f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
